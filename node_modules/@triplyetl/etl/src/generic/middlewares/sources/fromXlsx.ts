import { Etl, Middleware } from "../../index.js";
import { Record } from "../../Record.js";
import xlsx from "xlsx";
import { castArray, flatten, intersection, merge, isArray } from "lodash-es";
import { FileSourceInfo, SourceGetter } from "../../locations/sources/index.js";
import { addMwCallSiteToError, assertOneExtractorPerETL } from "../../../utils/index.js";
import { getErr } from "../../../utils/Error.js";
import { DuplicateColumnsError } from "./fromCsv.js";
import trimRecordFromSource from "./util/trimRecordFromSource.js";
import path from "path";
import * as fs from "fs";
xlsx.set_fs(fs);
export interface Options {
  /**
   * Only read from these sheet names. By default it reads all sheets
   */
  sheetNames: string[] | undefined;
  readOptions: xlsx.ParsingOptions;
  toJsonOptions: xlsx.Sheet2JSONOpts;
  skipEnrich?: boolean;
  skipTrim?: boolean;
  groupColumnsByName?: boolean;
}

const DEFAULTS: Options = {
  readOptions: {},
  // (#228 https://git.triply.cc/triply/etl/-/issues/228) currently NOT replacing empty cells with empty string, hence the key will never be part of the record when this key is empty
  // this results in fromXlsx not behaving like fromCsv when it comes to empty cells
  // toJsonOptions: { defval: "" },
  toJsonOptions: { raw: false },
  sheetNames: undefined,
  skipEnrich: false,
  skipTrim: false,
  groupColumnsByName: false,
};

// TODO @DocumentationTeam: add TS doc comment here
export default function fromXlsx(
  oneOrMoreSources: SourceGetter<"records", FileSourceInfo> | SourceGetter<"records", FileSourceInfo>[],
  _opts: Partial<Options> = {},
): Middleware {
  const opts: Options = merge({}, DEFAULTS, _opts);
  return addMwCallSiteToError(
    async function _fromXlsx(ctx, next) {
      assertOneExtractorPerETL(ctx.app, "fromXlsx");
      const sources = await ctx["_registerSources"](castArray(oneOrMoreSources), "records", {
        registerTotalProgress: true,
      });
      const xlsFiles = await Promise.all(
        sources.map(async (s) => {
          const localPath = path.resolve(await s.getLocalPath());
          try {
            const workbook = xlsx.readFile(localPath, opts.readOptions);
            return {
              source: s,
              workbook,
            };
          } catch (e) {
            if (e instanceof Error) {
              e.message = `Failed to parse XLSX ${localPath}: ${e.message}`;
            }
            throw e;
          }
        }),
      );
      const records = flatten(
        xlsFiles.map((xls) => {
          const sheetsToRead = opts.sheetNames
            ? intersection(opts.sheetNames, xls.workbook.SheetNames)
            : xls.workbook.SheetNames;
          ctx.source = xls.source;
          return flatten(
            sheetsToRead.map((sheetName) => {
              const enriched = !opts.skipEnrich
                ? {
                    $sheetName: sheetName,
                    $fileName: ctx.source?.name,
                  }
                : {};
              let xlsxArray = xlsx.utils.sheet_to_json(xls.workbook.Sheets[sheetName], { header: 1, defval: null });
              const xlsxHeaders = xlsxArray[0];
              if (isArray(xlsxArray) && isArray(xlsxHeaders) && new Set(xlsxHeaders).size !== xlsxHeaders.length) {
                if (opts.groupColumnsByName) {
                  if (xlsxHeaders.includes(null)) {
                    const e = getErr(new Error());
                    e.setMessage(
                      `Undefined header found in file, please name all headers in source file.\n Encountered headers: ${JSON.stringify(
                        xlsxHeaders,
                      )}`,
                    );
                    throw e;
                  }
                  const xlsxRows: Array<any> = xlsxArray.slice(1);
                  const dict: { [key: string]: any } = {};
                  xlsxHeaders.map((h) => (dict[h] = []));
                  xlsxHeaders.map((h, i) => xlsxRows.map((row) => dict[h].push(row[i])));
                  for (const key of Object.keys(dict)) {
                    dict[key] = dict[key].filter((n: any) => n !== null);
                    dict[key].length == 1 ? (dict[key] = dict[key][0]) : dict[key];
                  }
                  return {
                    ...enriched,
                    ...[dict],
                  };
                } else {
                  throw getErr(new DuplicateColumnsError(xls.source.name));
                }
              } else {
                let xlsxArray = xlsx.utils.sheet_to_json(xls.workbook.Sheets[sheetName], opts.toJsonOptions);
                return {
                  ...enriched,
                  ...xlsxArray,
                };
              }
            }),
          );
        }),
      ) as Record[];
      ctx.app.setTotalProgress(records.reduce((i, records) => i + Object.values(records).length, 0));
      for (const record of records) {
        for (const rowNum in record) {
          if (!isNaN(parseInt(rowNum))) {
            ctx.app.incrementProgress(1);
            const row: any = !opts.skipTrim ? trimRecordFromSource(record[rowNum]) : record[rowNum];
            const enriched: { [key: string]: number | string | undefined } = !opts.skipEnrich
              ? { $recordId: ctx.recordId + 1, $environment: Etl.environment }
              : {};
            enriched.$sheetName = record.$sheetName;
            enriched.$fileName = record.$fileName;
            await next({ ...enriched, ...row }, ctx.app.getNewStore());
          }
        }
      }
    },
    { sourceFuncName: "_fromXlsx" },
  );
}
