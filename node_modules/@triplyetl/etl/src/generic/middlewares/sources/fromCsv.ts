import { parse, Options as CsvParseOptions } from "csv-parse";
import * as zlib from "zlib";
import Pumpify from "pumpify";
import { FileSourceInfo, SourceGetter, SourceInfo } from "../../locations/sources/index.js";
import { castArray, has } from "lodash-es";
import { addMwCallSiteToError, assertOneExtractorPerETL, modifyStreamError } from "../../../utils/index.js";
import trimRecordFromSource from "./util/trimRecordFromSource.js";
import { isStatic, StaticString } from "../../../ratt/middlewares/assertions/term/str.js";
import { Etl, Middleware, Record } from "../../index.js";
import { QuerySourceGetter, isQuerySourceGetter } from "../../locations/sources/savedQuery.js";

export interface Options extends CsvParseOptions {
  skipTrim?: boolean;
  skipEnrich?: boolean;
  //this is for
  // groupColumnsByName: boolean
}

// group_columns_by_name
// the `trim` parameter is normaly only used by csv-parse to trim before and after quoted strings, not inside
// in this MW having this options set to true will also trim inside the key/value.
const DEFAULTS: Options = {
  columns: true,
  trim: true,
  bom: true,
  skip_empty_lines: true,
  skipTrim: false,
  skipEnrich: false,
};

export class DuplicateColumnsError extends Error {
  constructor(sourceFileInfo: SourceInfo | StaticString | string) {
    super();
    this.name = "DuplicateColumnsError";
    this.message = `The headers in the source data are not unique, use the "groupColumnsByName" option to avoid this error.${
      isStatic(sourceFileInfo) || typeof sourceFileInfo == "string"
        ? ` Source file: ${sourceFileInfo}`
        : ` Source file: ${sourceFileInfo.name}`
    }`;
  }
}
const checkDuplicateColumns = (csvHeader: string[], source: SourceInfo) => {
  if (new Set(csvHeader).size !== csvHeader.length) {
    throw new DuplicateColumnsError(source);
  } else {
    return csvHeader;
  }
};

// TODO @DocumentationTeam: add TS doc comment here
export default function fromCsv(
  oneOrMoreSources:
    | StaticString
    | SourceGetter<"records", FileSourceInfo>
    | SourceGetter<"records", FileSourceInfo>[]
    | QuerySourceGetter
    | QuerySourceGetter[],
  opts: Options = {},
): Middleware {
  const optsWithDefaults: Options = { ...DEFAULTS, ...opts };
  if (optsWithDefaults.skipTrim !== undefined) optsWithDefaults.trim = !optsWithDefaults.skipTrim;

  return addMwCallSiteToError(
    async function _fromCsv(ctx, next) {
      assertOneExtractorPerETL(ctx.app, "fromCsv");
      if (isStatic(oneOrMoreSources)) {
        const data = oneOrMoreSources.toString().trim().split("\n");
        if (!opts.groupColumnsByName) {
          let hasDuplicateHeaders = false;
          const testParser = parse({
            ...DEFAULTS,
            ...opts,
            columns: (csvHeader: string[]) => {
              hasDuplicateHeaders = csvHeader.length !== new Set(csvHeader).size;
              return csvHeader;
            },
          });

          testParser.on("readable", () => {
            testParser.emit("close");
          });
          data.forEach((line) => testParser.write(`${line}\n`));
          if (hasDuplicateHeaders) {
            throw new DuplicateColumnsError(oneOrMoreSources);
          }
          testParser.end();
        }

        const parser = parse(optsWithDefaults);
        ctx.app.setTotalProgress(data.length - 1);

        parser.on("readable", async function () {
          let row;
          while ((row = parser.read()) !== null) {
            ctx.app.incrementProgress(1);
            if (!optsWithDefaults.skipEnrich) {
              row.$recordId = ctx.recordId + 1;
              row.$environment = Etl.environment;
            }
            await next(optsWithDefaults.skipTrim ? row : trimRecordFromSource(row), ctx.app.getNewStore());
          }
        });
        data.forEach((line) => parser.write(`${line}\n`));
        parser.end();
      } else {
        const moreSources: SourceGetter<"records", FileSourceInfo>[] = castArray(oneOrMoreSources).filter(
          (source) => !isQuerySourceGetter(source),
        ) as any as SourceGetter<"records", FileSourceInfo>[];
        const sources = await ctx["_registerSources"](moreSources, "records", {
          registerTotalProgress: true,
        });

        const querySources: QuerySourceGetter[] = castArray(oneOrMoreSources).filter((source) =>
          isQuerySourceGetter(source),
        ) as any as QuerySourceGetter[];

        const enrich = (record: Partial<Record>, filename: string) => {
          if (!optsWithDefaults.skipEnrich) {
            record.$recordId = ctx.recordId + 1;
            record.$fileName = filename;
            record.$environment = Etl.environment;
          }
        };

        for (const source of querySources) {
          const src = (await source.get(ctx.app, "records")).pop();
          if (src !== undefined) {
            if (has(src, "getBoolean")) {
              const askSource = (await source.get(ctx.app, "boolean")).pop();
              const record: Partial<Record> = { boolean: await askSource?.getBoolean() };
              enrich(record, await src.id());
              await next(optsWithDefaults.skipTrim ? record : trimRecordFromSource(record), ctx.app.getNewStore());
            } else {
              await src.getAsyncIterable().then(async (bindings) => {
                for await (const binding of bindings) {
                  ctx.app.incrementProgress(1);
                  const record: Partial<Record> = { ...binding };
                  enrich(record, await src.id());
                  await next(optsWithDefaults.skipTrim ? record : trimRecordFromSource(record), ctx.app.getNewStore());
                }
              });
            }
          }
        }

        for (const source of sources) {
          ctx.source = source;
          const parser = parse({
            ...optsWithDefaults,
            columns: opts.groupColumnsByName
              ? opts.columns || DEFAULTS.columns || true
              : (csvHeader: string[]) => checkDuplicateColumns(csvHeader, source),
          });
          const rs = await source.getStream();
          rs.on("data", (d) => {
            ctx.app.incrementProgress(d.length);
          });
          const stream =
            (await source.compression()) === "gz"
              ? new Pumpify.obj(rs, zlib.createGunzip(), parser)
              : new Pumpify.obj(rs, parser);

          modifyStreamError(stream, (e) => {
            if (e.name === "DuplicateColumnsError") return e;
            e.message = `Failed to parse ${source.name}: ${e.message}`;
            return e;
          });

          for await (const row of stream) {
            if (!optsWithDefaults.skipEnrich) {
              row.$recordId = ctx.recordId + 1;
              row.$fileName = source.name;
              row.$environment = Etl.environment;
            }
            await next(optsWithDefaults.skipTrim ? row : trimRecordFromSource(row), ctx.app.getNewStore());
          }
        }
      }
    },
    { sourceFuncName: "_fromCsv" },
  );
}
