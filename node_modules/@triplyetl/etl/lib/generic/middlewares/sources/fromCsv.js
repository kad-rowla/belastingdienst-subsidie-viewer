import { parse } from "csv-parse";
import * as zlib from "zlib";
import Pumpify from "pumpify";
import { castArray, has } from "lodash-es";
import { addMwCallSiteToError, assertOneExtractorPerETL, modifyStreamError } from "../../../utils/index.js";
import trimRecordFromSource from "./util/trimRecordFromSource.js";
import { isStatic } from "../../../ratt/middlewares/assertions/term/str.js";
import { Etl } from "../../index.js";
import { isQuerySourceGetter } from "../../locations/sources/savedQuery.js";
// group_columns_by_name
// the `trim` parameter is normaly only used by csv-parse to trim before and after quoted strings, not inside
// in this MW having this options set to true will also trim inside the key/value.
const DEFAULTS = {
    columns: true,
    trim: true,
    bom: true,
    skip_empty_lines: true,
    skipTrim: false,
    skipEnrich: false,
};
export class DuplicateColumnsError extends Error {
    constructor(sourceFileInfo) {
        super();
        this.name = "DuplicateColumnsError";
        this.message = `The headers in the source data are not unique, use the "groupColumnsByName" option to avoid this error.${isStatic(sourceFileInfo) || typeof sourceFileInfo == "string"
            ? ` Source file: ${sourceFileInfo}`
            : ` Source file: ${sourceFileInfo.name}`}`;
    }
}
const checkDuplicateColumns = (csvHeader, source) => {
    if (new Set(csvHeader).size !== csvHeader.length) {
        throw new DuplicateColumnsError(source);
    }
    else {
        return csvHeader;
    }
};
// TODO @DocumentationTeam: add TS doc comment here
export default function fromCsv(oneOrMoreSources, opts = {}) {
    const optsWithDefaults = { ...DEFAULTS, ...opts };
    if (optsWithDefaults.skipTrim !== undefined)
        optsWithDefaults.trim = !optsWithDefaults.skipTrim;
    return addMwCallSiteToError(async function _fromCsv(ctx, next) {
        assertOneExtractorPerETL(ctx.app, "fromCsv");
        if (isStatic(oneOrMoreSources)) {
            const data = oneOrMoreSources.toString().trim().split("\n");
            if (!opts.groupColumnsByName) {
                let hasDuplicateHeaders = false;
                const testParser = parse({
                    ...DEFAULTS,
                    ...opts,
                    columns: (csvHeader) => {
                        hasDuplicateHeaders = csvHeader.length !== new Set(csvHeader).size;
                        return csvHeader;
                    },
                });
                testParser.on("readable", () => {
                    testParser.emit("close");
                });
                data.forEach((line) => testParser.write(`${line}\n`));
                if (hasDuplicateHeaders) {
                    throw new DuplicateColumnsError(oneOrMoreSources);
                }
                testParser.end();
            }
            const parser = parse(optsWithDefaults);
            ctx.app.setTotalProgress(data.length - 1);
            parser.on("readable", async function () {
                let row;
                while ((row = parser.read()) !== null) {
                    ctx.app.incrementProgress(1);
                    if (!optsWithDefaults.skipEnrich) {
                        row.$recordId = ctx.recordId + 1;
                        row.$environment = Etl.environment;
                    }
                    await next(optsWithDefaults.skipTrim ? row : trimRecordFromSource(row), ctx.app.getNewStore());
                }
            });
            data.forEach((line) => parser.write(`${line}\n`));
            parser.end();
        }
        else {
            const moreSources = castArray(oneOrMoreSources).filter((source) => !isQuerySourceGetter(source));
            const sources = await ctx["_registerSources"](moreSources, "records", {
                registerTotalProgress: true,
            });
            const querySources = castArray(oneOrMoreSources).filter((source) => isQuerySourceGetter(source));
            const enrich = (record, filename) => {
                if (!optsWithDefaults.skipEnrich) {
                    record.$recordId = ctx.recordId + 1;
                    record.$fileName = filename;
                    record.$environment = Etl.environment;
                }
            };
            for (const source of querySources) {
                const src = (await source.get(ctx.app, "records")).pop();
                if (src !== undefined) {
                    if (has(src, "getBoolean")) {
                        const askSource = (await source.get(ctx.app, "boolean")).pop();
                        const record = { boolean: await askSource?.getBoolean() };
                        enrich(record, await src.id());
                        await next(optsWithDefaults.skipTrim ? record : trimRecordFromSource(record), ctx.app.getNewStore());
                    }
                    else {
                        await src.getAsyncIterable().then(async (bindings) => {
                            for await (const binding of bindings) {
                                ctx.app.incrementProgress(1);
                                const record = { ...binding };
                                enrich(record, await src.id());
                                await next(optsWithDefaults.skipTrim ? record : trimRecordFromSource(record), ctx.app.getNewStore());
                            }
                        });
                    }
                }
            }
            for (const source of sources) {
                ctx.source = source;
                const parser = parse({
                    ...optsWithDefaults,
                    columns: opts.groupColumnsByName
                        ? opts.columns || DEFAULTS.columns || true
                        : (csvHeader) => checkDuplicateColumns(csvHeader, source),
                });
                const rs = await source.getStream();
                rs.on("data", (d) => {
                    ctx.app.incrementProgress(d.length);
                });
                const stream = (await source.compression()) === "gz"
                    ? new Pumpify.obj(rs, zlib.createGunzip(), parser)
                    : new Pumpify.obj(rs, parser);
                modifyStreamError(stream, (e) => {
                    if (e.name === "DuplicateColumnsError")
                        return e;
                    e.message = `Failed to parse ${source.name}: ${e.message}`;
                    return e;
                });
                for await (const row of stream) {
                    if (!optsWithDefaults.skipEnrich) {
                        row.$recordId = ctx.recordId + 1;
                        row.$fileName = source.name;
                        row.$environment = Etl.environment;
                    }
                    await next(optsWithDefaults.skipTrim ? row : trimRecordFromSource(row), ctx.app.getNewStore());
                }
            }
        }
    }, { sourceFuncName: "_fromCsv" });
}
//# sourceMappingURL=fromCsv.js.map