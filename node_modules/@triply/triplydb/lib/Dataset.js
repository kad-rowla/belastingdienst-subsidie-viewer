import fs from "fs-extra";
import { wait } from "./utils/index.js";
import debug from "debug";
import Service from "./Service.js";
const log = debug("triply:triplydb-js:upload");
import * as tus from "@triply/tus-js-client";
import md5 from "md5";
import { tmpdir } from "os";
import path from "path";
import * as zlib from "zlib";
import * as n3 from "n3";
import pumpify from "pumpify";
import { fromPairs, toPairs, pick, size, uniq, zipObject } from "lodash-es";
import { TriplyDbJsError, getErr, IncompatibleError } from "./utils/Error.js";
import { _get, _delete, _patch, _post, handleFetchAsStream } from "./RequestHandler.js";
import AsyncIteratorHelper from "./utils/AsyncIteratorHelper.js";
import Asset from "./Asset.js";
import Graph from "./Graph.js";
import { stringify as stringifyQueryObj } from "query-string";
import statuses from "http-status-codes";
import NDEDatasetRegister from "./utils/NDEDatasetRegister.js";
export default class Dataset {
    constructor(app, owner, datasetName, datasetInfo) {
        this.type = "Dataset";
        this.nde = {
            datasetregister: {
                /**
                 * Register this dataset with the [NDE Dataset register](https://datasetregister.netwerkdigitaalerfgoed.nl/)
                 *
                 * @param rejectOnValidationError an optional boolean (default = true) indicating that SHACL validation errors should throw an Error.
                 *                                If false, the function will not throw but return a Stroe containng the SHACL validation report.
                 * @example
                 * ```ts
                 * App.get(token)
                 *   .getAccount(accountName)
                 *   .then(account => account.getDataset(datasetName))
                 *   .then(dataset => dataset.nde.datasetregister.submit())
                 * ```
                 */
                submit: async (rejectOnValidationError) => NDEDatasetRegister(this, "submit", rejectOnValidationError),
                /**
                 * Validate this dataset against the [NDE Dataset register](https://datasetregister.netwerkdigitaalerfgoed.nl/)
                 *
                 * @param rejectOnValidationError an optional boolean (default = true) indicating that SHACL validation errors should throw an Error.
                 *                                If false, the function will not throw but return a Stroe containng the SHACL validation report.
                 * @example
                 * ```ts
                 * App.get(token)
                 *   .getAccount(accountName)
                 *   .then(account => account.getDataset(datasetName))
                 *   .then(dataset => dataset.nde.datasetregister.submit())
                 * ```
                 */
                validate: async (rejectOnValidationError) => NDEDatasetRegister(this, "validate", rejectOnValidationError),
            },
        };
        this._app = app;
        this._name = datasetName;
        this._owner = owner;
        this._info = datasetInfo;
    }
    getServices() {
        return new AsyncIteratorHelper({
            potentialFutureError: getErr(`Failed to get services`),
            getErrorMessage: async () => `Failed to get services for dataset ${await this._getDatasetNameWithOwner()}.`,
            app: this._app,
            getUrl: async () => this._app["_config"].url + (await this._getDatasetPath("/services")),
            mapResult: async (info) => {
                return new Service({
                    app: this._app,
                    dataset: this,
                    name: info.name,
                    type: info.type,
                    config: info.config,
                });
            },
        });
    }
    async getService(serviceName) {
        const sv2Metadata = await _get({
            errorWithCleanerStack: getErr(`Failed to get service '${serviceName}' from dataset ${this["_name"]}.`),
            app: this._app,
            path: `${await this._getDatasetPath("/services/")}${serviceName}`,
        });
        return new Service({
            app: this._app,
            dataset: this,
            name: sv2Metadata.name,
            type: sv2Metadata.type,
            config: sv2Metadata.config,
        });
    }
    async clear(resourceType, ...rest) {
        await Promise.all(uniq(rest.concat(resourceType)).map(async (typeToClear) => {
            if (typeToClear === "graphs") {
                return _delete({
                    errorWithCleanerStack: getErr(`Failed to remove all graphs for ${await this._getDatasetNameWithOwner()}.`),
                    app: this._app,
                    path: await this._getDatasetPath("/graphs"),
                    expectedResponseBody: "empty",
                });
            }
            else if (typeToClear === "assets") {
                for await (let asset of this.getAssets())
                    await asset?.delete();
            }
            else if (typeToClear === "services") {
                for await (let service of this.getServices())
                    await service?.delete();
            }
            else {
                throw getErr(`Unrecognized resource type: ${typeToClear}`);
            }
        }));
        await this.getInfo(true);
        return this;
    }
    async getGraph(graphNameOrIri) {
        const graphName = typeof graphNameOrIri === "string" ? graphNameOrIri : graphNameOrIri.value;
        for await (let graph of this.getGraphs()) {
            if (!graph)
                break;
            if ((await graph.getInfo()).graphName === graphName)
                return graph;
        }
        throw getErr(`Graph '${graphName}' not found in dataset ${await this._getDatasetNameWithOwner()}`);
    }
    async deleteGraph(graphNameOrIri) {
        const graphName = typeof graphNameOrIri === "string" ? graphNameOrIri : graphNameOrIri.value;
        const graph = await this.getGraph(graphName);
        await graph.delete();
    }
    _setInfo(info) {
        this._info = info;
        this._name = info.name;
        return this;
    }
    async _getDatasetPath(additionalPath) {
        const ownerName = (await this._owner.getInfo()).accountName;
        return "/datasets/" + ownerName + "/" + this._name + (additionalPath || "");
    }
    async _getDatasetNameWithOwner() {
        const ownerName = (await this._owner.getInfo()).accountName;
        return `${ownerName}/${this._name}`;
    }
    async getInfo(refresh = false) {
        if (!refresh && this._info)
            return this._info;
        const info = await _get({
            errorWithCleanerStack: getErr(`Failed to get dataset information for ${await this._getDatasetNameWithOwner()}.`),
            app: this._app,
            path: await this._getDatasetPath(),
        });
        this._setInfo(info);
        return info;
    }
    async getAsset(assetName, versionNumber) {
        return new Asset(this, (await _get({
            errorWithCleanerStack: getErr(`Failed to get asset ${assetName} from dataset ${await this._getDatasetNameWithOwner()}.`),
            app: this._app,
            path: await this._getDatasetPath("/assets"),
            query: { fileName: assetName },
        })), versionNumber);
    }
    getAssets() {
        return new AsyncIteratorHelper({
            potentialFutureError: getErr(`Failed to get assets`),
            getErrorMessage: async () => `Failed to get assets of dataset ${await this._getDatasetNameWithOwner()}.`,
            app: this._app,
            getUrl: async () => this._app["_config"].url + (await this._getDatasetPath("/assets")),
            mapResult: async (assetInfo) => new Asset(this, assetInfo),
        });
    }
    getGraphs() {
        return new AsyncIteratorHelper({
            potentialFutureError: getErr(`Failed to get graphs`),
            getErrorMessage: async () => `Failed to get graphs of dataset ${await this._getDatasetNameWithOwner()}.`,
            app: this._app,
            getUrl: async () => this._app["_config"].url + (await this._getDatasetPath("/graphs")),
            mapResult: async (info) => new Graph(this, info),
        });
    }
    // Extension comes from a path.parse method, so we can trust it to start with a `.`
    async _getDownloadPath(extension, graph) {
        const dsPath = `${await this._getDatasetPath()}`;
        return `${dsPath}/download${extension}` + (graph ? `?graph=${encodeURIComponent(graph["_info"].graphName)}` : "");
    }
    async graphsToFile(destinationPath, opts) {
        const parsedPath = path.parse(destinationPath);
        if (!(await fs.pathExists(path.resolve(parsedPath.dir)))) {
            throw getErr(`Directory doesn't exist: ${parsedPath.dir}`);
        }
        let extension = parsedPath.ext;
        let storeCompressed;
        if (typeof opts?.compressed === "boolean") {
            storeCompressed = opts.compressed;
        }
        else {
            storeCompressed = extension === ".gz";
        }
        if (extension === ".gz") {
            extension = path.extname(parsedPath.name);
        }
        const stream = new pumpify(await this.graphsToStream("compressed", { graph: opts?.graph, extension }), 
        // we always download compressed version. Decompress unless the user saves as *.gz
        ...(storeCompressed ? [] : [zlib.createGunzip()]), fs.createWriteStream(destinationPath));
        await new Promise((resolve, reject) => {
            stream.on("error", reject);
            stream.on("finish", resolve);
        });
    }
    async graphsToStream(type, opts) {
        const stream = await handleFetchAsStream("GET", {
            app: this._app,
            path: await this._getDownloadPath((opts?.extension ?? ".trig") + ".gz", opts?.graph),
            errorWithCleanerStack: getErr(opts?.graph
                ? `Failed to download graph ${opts?.graph["_info"].graphName}`
                : `Failed to download graphs of dataset ${await this._getDatasetNameWithOwner()}.`),
        });
        if (type === "compressed") {
            return stream;
        }
        else {
            return new pumpify.obj(stream, zlib.createGunzip(), new n3.StreamParser());
        }
    }
    async graphsToStore(graph) {
        const store = new n3.Store();
        const stream = await this.graphsToStream("rdf-js", { graph });
        await new Promise((resolve, reject) => {
            store.import(stream).on("finish", resolve).on("error", reject);
        });
        return store;
    }
    async importFromDataset(fromDataset, args) {
        let graphs;
        if (args && args.graphMap && args.graphNames) {
            throw getErr("please use either the property 'graphMap' or 'graphNames', but not both together");
        }
        else if (args && args.graphMap) {
            graphs = {};
            for (let graph in args.graphMap) {
                const key = graph;
                const value = args.graphMap[graph];
                if (value instanceof Graph) {
                    graphs[key] = await value.getInfo().then((g) => g.graphName);
                }
                else if (typeof value === "string") {
                    graphs[key] = value;
                }
                else {
                    graphs[key] = value.value;
                }
            }
        }
        else if (args && args.graphNames) {
            const graphNames = await Promise.all(args.graphNames.map((graph) => {
                if (graph instanceof Graph) {
                    return graph.getInfo().then((g) => g.graphName);
                }
                else if (typeof graph === "string") {
                    return graph;
                }
                else {
                    return graph.value;
                }
            }));
            graphs = zipObject(graphNames, graphNames);
        }
        const overwrite = !!args?.overwrite;
        if (overwrite && !(await this._app.isCompatible("2.2.7"))) {
            throw new IncompatibleError("Overwriting graphs is only supported by TriplyDB API version 2.2.7 or greater");
        }
        if (!graphs) {
            // import all the graphs, keeping the original names.
            const graphNames = await Promise.all((await fromDataset.getGraphs().toArray()).map((g) => g.getInfo().then((g) => g.graphName)));
            graphs = zipObject(graphNames, graphNames);
        }
        const fromDsInfo = await fromDataset.getInfo();
        const graphsToImport = [];
        for (const fromGraph in graphs) {
            graphsToImport.push({
                from: fromGraph,
                to: graphs[fromGraph],
                overwrite: !!overwrite,
            });
        }
        return _patch({
            errorWithCleanerStack: getErr(`Tried importing from dataset ${await fromDataset._getDatasetNameWithOwner()}. Failed to write the changes to ${await this._getDatasetNameWithOwner()}.`),
            app: this._app,
            path: await this._getDatasetPath("/imports"),
            data: [
                {
                    dataset: {
                        ownerName: fromDsInfo.owner.accountName,
                        datasetName: fromDsInfo.name,
                    },
                    graphs: graphsToImport,
                },
            ],
        });
    }
    async update(config) {
        this._setInfo(await _patch({
            errorWithCleanerStack: getErr(`Failed to update dataset information of ${await this._getDatasetNameWithOwner()}.`),
            app: this._app,
            path: await this._getDatasetPath(),
            data: config,
        }));
        return this;
    }
    async copy(toAccountName, newDatasetName) {
        const newDs = await _post({
            errorWithCleanerStack: getErr(`Failed to copy dataset ${await this._getDatasetNameWithOwner()} to ${toAccountName}.`),
            app: this._app,
            path: await this._getDatasetPath("/copy"),
            data: { toAccount: toAccountName, name: newDatasetName },
        });
        const toAccount = await this._app.getAccount(toAccountName);
        return (await toAccount.getDataset(newDs.name))._setInfo(newDs);
    }
    async renameGraph(from, to) {
        const graph = await this.getGraph(from);
        await graph.rename(to);
        return graph;
    }
    async delete() {
        await _delete({
            errorWithCleanerStack: getErr(`Failed to delete dataset ${await this._getDatasetNameWithOwner()}.`),
            app: this._app,
            path: await this._getDatasetPath(),
            expectedResponseBody: "empty",
        });
        this._info = undefined;
        this._lastJob = undefined;
    }
    async setAvatar(pathBufferOrFile) {
        const info = await this.getInfo();
        await _post({
            errorWithCleanerStack: getErr(`Failed to set avatar of dataset ${await this._getDatasetNameWithOwner()}.`),
            app: this._app,
            path: "/imgs/avatars/d/" + info.id,
            attach: { avatar: pathBufferOrFile },
        });
        await this.getInfo(true);
        return this;
    }
    _throwIfJobRunning(dsId) {
        if (datasetsWithOngoingJob[dsId]) {
            throw getErr("There is already an ongoing job for this dataset. Await that one first.");
        }
    }
    async importFromFiles(files, defaultsConfig) {
        const dsId = await this.getInfo().then((info) => info.id);
        this._throwIfJobRunning(dsId);
        try {
            datasetsWithOngoingJob[dsId] = true;
            if (defaultsConfig?.overwriteAll && !(await this._app.isCompatible("2.2.7"))) {
                throw new IncompatibleError("Overwriting graphs is only supported by TriplyDB API version 2.2.7 or greater");
            }
            const job = new JobUpload({
                app: this._app,
                ...defaultsConfig,
                datasetPath: await this._getDatasetPath(),
                datasetNameWithOwner: await this._getDatasetNameWithOwner(),
            });
            this._lastJob = await job.create();
            await this._lastJob.uploadFiles(files);
            await this._lastJob.exec();
            await this.getInfo(true); // This way we update things like the ds statement count
            return this;
        }
        finally {
            delete datasetsWithOngoingJob[dsId];
        }
    }
    async importFromStore(store, opts) {
        /**
         * We're writing the store to disk and then uploading
         * This can be improved at a later moment in time by uploading from memory using Buffer
         */
        const quads = store.getQuads(null, null, null, null);
        const quadsString = new n3.Writer({ format: "n-quads" }).quadsToString(quads);
        const tmpFile = path.resolve(tmpdir(), `triplydb-${md5(quadsString)}.nq`);
        await fs.writeFile(tmpFile, quadsString, "utf-8");
        return this.importFromFiles([tmpFile], opts);
    }
    async importFromUrls(urls, defaultConfig) {
        const dsId = await this.getInfo().then((info) => info.id);
        try {
            this._throwIfJobRunning(dsId);
            datasetsWithOngoingJob[dsId] = true;
            if (defaultConfig?.overwriteAll && !(await this._app.isCompatible("2.2.7"))) {
                throw new IncompatibleError("Overwriting graphs is only supported by TriplyDB API version 2.2.7 or greater");
            }
            const ownerName = (await this._owner.getInfo()).accountName;
            let info = await _post({
                errorWithCleanerStack: getErr(`Failed to delete import from ${urls.length} URLs in dataset ${await this._getDatasetNameWithOwner()}.`),
                app: this._app,
                path: "/datasets/" + ownerName + "/" + this._name + "/jobs",
                data: {
                    ...defaultConfig,
                    type: "download",
                    downloadUrls: urls,
                },
            });
            const jobUrl = `${this._app["_config"].url}${await this._getDatasetPath("/jobs/" + info.jobId)}`;
            info = await waitForJobToFinish(this._app, jobUrl, (await this.getInfo()).id);
            await this.getInfo(true); //Sync info so the ds metadata is up to date with imported statements
            return this;
        }
        finally {
            delete datasetsWithOngoingJob[dsId];
        }
    }
    async describe(iri) {
        const iriString = typeof iri === "string" ? iri : iri.value;
        const buffer = await _get({
            app: this._app,
            path: await this._getDatasetPath("/describe.nt"),
            query: {
                resource: iriString,
            },
            expectedResponseBody: "buffer",
            errorWithCleanerStack: getErr(`Failed to describe '${iri}' of ${await this._getDatasetNameWithOwner()}.`),
        });
        return new n3.Parser().parse(buffer.toString());
    }
    getStatements(payload) {
        return new AsyncIteratorHelper({
            potentialFutureError: getErr(`Failed to get statements`),
            getErrorMessage: async () => `Failed to get statements of dataset ${await this._getDatasetNameWithOwner()}.`,
            app: this._app,
            mapResult: async (info) => info,
            getUrl: async () => this._app["_config"].url +
                (await this._getDatasetPath("/statements")) +
                "?" +
                stringifyQueryObj({ limit: 50, ...pick(payload, "subject", "predicate", "object", "graph") }),
        });
    }
    async uploadAsset(fileOrPath, assetName) {
        if (!assetName) {
            if (typeof fileOrPath === "string") {
                assetName = fileOrPath;
            }
            else {
                assetName = fileOrPath.name;
            }
        }
        let assetAlreadyExists = false;
        try {
            await this.getAsset(assetName);
            assetAlreadyExists = true; //if it doesnt exist, it would have thrown
        }
        catch (e) {
            if (e instanceof TriplyDbJsError && e.statusCode === 404) {
                //this is fine
            }
            else {
                throw e;
            }
        }
        if (assetAlreadyExists) {
            throw new TriplyDbJsError(`Tried to add asset '${assetName}' to dataset ${await this._getDatasetNameWithOwner()}, but an asset with that name already exists.`).setStatusCode(statuses.CONFLICT);
        }
        return new Asset(this, await Asset["uploadAsset"]({ fileOrPath, assetName, dataset: this }));
    }
    async addService(name, opts) {
        return new Service({
            app: this._app,
            dataset: this,
            name,
            type: opts ? opts.type : "virtuoso",
            config: opts?.config || undefined,
        }).create();
    }
    async ensureService(name, args) {
        try {
            const foundService = await this.getService(name);
            return foundService;
        }
        catch (e) {
            if (e.statusCode !== 404)
                throw e;
            return this.addService(name, args);
        }
    }
    async addPrefixes(newPrefixes) {
        const asPairs = toPairs(newPrefixes);
        await _patch({
            errorWithCleanerStack: getErr(`Failed to add ${size(newPrefixes)} prefixes to dataset ${await this._getDatasetNameWithOwner()}.`),
            app: this._app,
            path: await this._getDatasetPath("/prefixes"),
            data: asPairs.map(([key, value]) => {
                let prefixIri;
                if (typeof value === "string") {
                    prefixIri = value;
                }
                else if (typeof value === "function") {
                    prefixIri = value("").value;
                }
                else {
                    prefixIri = value.value;
                }
                return { prefixLabel: key, iri: prefixIri, scope: "local" };
            }),
        });
        return this.getPrefixes(true);
    }
    /**
     * Remove prefixes defined at the dataset level
     */
    async removePrefixes(prefixLabels) {
        const dsPath = await this._getDatasetPath();
        await Promise.all(prefixLabels.map(async (p) => _delete({
            errorWithCleanerStack: getErr(`Failed to delete prefix ${p} from dataset ${await this._getDatasetNameWithOwner()}.`),
            app: this._app,
            path: dsPath + "/prefixes/" + p,
            expectedResponseBody: "empty",
        }).catch((e) => {
            if (e instanceof TriplyDbJsError && e.statusCode === 404) {
                //fine
                return;
            }
            throw e;
        })));
        return this.getPrefixes(true);
    }
    /**
     * Getting _all_ prefixes (not just the dataset-scoped ones)
     */
    async getPrefixes(refresh = false) {
        if (refresh || !this._allPrefixes) {
            const prefixes = await _get({
                errorWithCleanerStack: getErr(`Failed to get prefixes of dataset ${await this._getDatasetNameWithOwner()}.`),
                app: this._app,
                path: await this._getDatasetPath("/prefixes"),
            });
            this._allPrefixes = fromPairs(prefixes.map((p) => [p.prefixLabel, p.iri]));
        }
        return this._allPrefixes;
    }
}
const datasetsWithOngoingJob = {};
async function waitForJobToFinish(app, jobUrl, dsId) {
    let waitFor = 100; //100ms
    const check = async () => {
        const info = await _get({
            errorWithCleanerStack: getErr(`Failed to get upload job status`).addContext({ jobUrl }),
            app: app,
            url: jobUrl,
        });
        if (info.status === "error")
            throw getErr(info.error?.message || "Failed to upload file");
        if (info.status === "canceled" || info.status === "finished")
            return info;
        await wait(waitFor);
        if (waitFor < 10 * 1000)
            waitFor = waitFor * 2; //max 10 seconds
        return check();
    };
    try {
        return await check();
    }
    finally {
        delete datasetsWithOngoingJob[dsId];
    }
}
const maxJobUploadWindow = 8;
export class JobUpload {
    constructor(conf) {
        this.urlMapper = (url) => url;
        this._config = conf;
        if (conf.app["_info"]?.apiUrl !== conf.app.getConfig().url) {
            this.urlMapper = (url) => {
                const u = new URL(url);
                return conf.app.getConfig().url.replace(/\/+$/g, "") + u.pathname + u.search;
            };
        }
    }
    getJobUrl() {
        return this.jobUrl;
    }
    async create() {
        const { app, datasetPath, datasetNameWithOwner: _ignored, ...data } = this._config;
        this._info = await _post({
            errorWithCleanerStack: getErr(`Failed to create job for dataset ${this._config.datasetNameWithOwner}.`),
            app: app,
            path: datasetPath + "/jobs",
            data,
        });
        this.jobUrl = `${app["_config"].url}${datasetPath}/jobs/${this._info.jobId}`;
        return this;
    }
    info() {
        return this._info;
    }
    async uploadFile(fileOrPath) {
        let rs;
        let fileSize;
        let fileName;
        if (typeof fileOrPath === "string") {
            // We can't use 'fs' outside of node, let's warn the dev about this
            if (fs.createReadStream === undefined) {
                throw getErr('"fs" is not loaded in this environment, use a "File" instead');
            }
            rs = fs.createReadStream(fileOrPath);
            fileSize = (await fs.stat(fileOrPath)).size;
            fileName = fileOrPath;
        }
        else {
            rs = fileOrPath;
            fileSize = fileOrPath.size;
            fileName = fileOrPath.name;
        }
        return new Promise((resolve, reject) => {
            const upload = new tus.Upload(rs, {
                endpoint: this.jobUrl + "/add",
                metadata: {
                    filename: fileName,
                },
                headers: {
                    Authorization: "Bearer " + this._config.app["_config"].token,
                },
                mapUrl: this.urlMapper,
                chunkSize: 5 * 1024 * 1024,
                uploadSize: fileSize,
                retryDelays: [2000, 5000, 10000, 40000, 50000],
                onError: reject,
                onProgress: function (_bytesUploaded, _bytesTotal) { },
                onSuccess: function () {
                    log("finished file " + fileOrPath);
                    resolve();
                },
            });
            upload.start();
        });
    }
    async uploadFiles(files) {
        const promises = [];
        const getFromStack = async () => {
            const file = files.pop();
            if (!file)
                return; //we're done
            await this.uploadFile(file);
            return getFromStack();
        };
        for (let i = 0; i < Math.min(maxJobUploadWindow, files.length); i++) {
            promises.push(getFromStack());
        }
        await Promise.all(promises);
        await this.refresh();
        return this;
    }
    async refresh() {
        if (!this.jobUrl)
            throw getErr("Cannot refresh uninstantiated job");
        this._info = await _get({
            errorWithCleanerStack: getErr(`Failed to get job information for dataset ${this._config.datasetNameWithOwner}.`).addContext({
                jobUrl: this.jobUrl,
            }),
            url: this.jobUrl,
            app: this._config.app,
        });
    }
    async exec() {
        if (!this.jobUrl)
            throw getErr("Cannot start uninstantiated job");
        this._info = await _post({
            errorWithCleanerStack: getErr(`Failed to start job in dataset ${this._config.datasetNameWithOwner}.`).addContext({
                jobUrl: this.jobUrl,
            }),
            app: this._config.app,
            url: this.jobUrl + "/start",
        });
        this._info = await waitForJobToFinish(this._config.app, this.jobUrl, this._info.datasetId);
    }
}
//# sourceMappingURL=Dataset.js.map